# -*- coding: utf-8 -*-
import torch
import numpy as np
from PIL import Image
import io
import base64
import requests
import json
import re
from typing import Tuple

# ä»å…±äº«æ–‡ä»¶ä¸­å¯¼å…¥é€šç”¨é…ç½®å’Œå‡½æ•°
from .common import SERVICE_CONFIG, clean_text, load_preset_configs

class AIPromptRefiner:
    """
    AIæç¤ºè¯ä¼˜åŒ–å·¥å…·ï¼Œä¸€ä¸ªæ”¯æŒå¤šç§å¤§æ¨¡å‹æœåŠ¡çš„ComfyUIèŠ‚ç‚¹ã€‚
    """
    
    NEGATIVE_PROMPTS = {
        "åŸºç¡€æ¨¡æ¿": "worst quality, low quality, watermark, signature, text, error, blurry",
        "é€šç”¨é«˜è´¨é‡": "lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name, deformed, distorted, disfigured",
        "å®Œç¾äººåƒ(é€šç”¨)": "bad hands, bad fingers, missing fingers, extra fingers, ugly, deformed, noisy, blurry, distorted, grainy, extra limbs, missing limbs, disconnected limbs, malformed hands, mutated hands, poorly drawn hands, extra head, malformed, (mutated hands and fingers:1.4)",
        "å®Œç¾äººåƒ(å†™å®)": "deformed, distorted, disfigured, poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, (mutated hands and fingers:1.4), disconnected limbs, mutation, mutated, ugly, disgusting, blurry, amputation, realisticface",
        "åŠ¨æ¼«ä¼˜åŒ–": "bad hands, bad fingers, missing fingers, extra fingers, ugly, deformed, noisy, blurry, distorted, grainy"
    }

    @classmethod
    def INPUT_TYPES(cls):
        preset_configs = load_preset_configs()
        preset_names = [config["name"] for config in preset_configs]
        if not preset_names:
            preset_names = ["æœªæ‰¾åˆ°é…ç½®,è¯·æ£€æŸ¥config.json"]

        style_options = [
            "é€šç”¨", "æ‘„å½±", "å†™å®", "åŠ¨æ¼«", "3Dæ¨¡å‹", "ç”µå½±æ„Ÿ", "æ¦‚å¿µè‰ºæœ¯", 
            "å»ºç­‘è®¾è®¡", "å¥‡å¹»", "èµ›åšæœ‹å…‹", "è’¸æ±½æœ‹å…‹", "æ°´å¢¨ç”»", 
            "æ²¹ç”»", "æ°´å½©ç”»", "ç´ æ", "åƒç´ è‰ºæœ¯", "ä½å¤šè¾¹å½¢", "ç®€çº¦", "å¤å¤"
        ]
        
        target_model_options = [
            "é€šç”¨", 
            "SDXL",
            "Flux"
        ]

        return {
            "required": {
                "prompt": ("STRING", {"multiline": True, "default": "a cute cat", "tooltip": "è¾“å…¥åŸºç¡€çš„æ–‡æœ¬æç¤ºè¯ã€‚"}),
                "config_selection": (preset_names, {"default": preset_names[0] if preset_names else "", "tooltip": "ä»æ‚¨çš„ config.json æ–‡ä»¶ä¸­é€‰æ‹©ä¸€ä¸ªé¢„è®¾é…ç½®ã€‚"}),
                "target_model": (target_model_options, {"default": "Flux", "tooltip": "é€‰æ‹©ç›®æ ‡æ¨¡å‹çš„æç¤ºè¯é£æ ¼ï¼Œä»¥è·å¾—æœ€ä½³æ•ˆæœã€‚"}),
                "strict_mode": ("BOOLEAN", {"default": True, "label_on": "è§„èŒƒè¾“å‡º", "label_off": "åˆ›æ„æ¨¡å¼", "tooltip": "å»ºè®®å¼€å¯ï¼Œä»¥ç¡®ä¿AIè¿”å›æ ¼å¼æ­£ç¡®ï¼Œä¾¿äºè§£æã€‚"}),
                "style": (style_options, {"default": "é€šç”¨", "tooltip": "é€‰æ‹©æœ€ç»ˆæç¤ºè¯çš„è‰ºæœ¯é£æ ¼ã€‚"}),
                "detail_level": (["åŸºç¡€", "è¯¦ç»†", "æå…¶è¯¦ç»†"], {"default": "åŸºç¡€", "tooltip": "æ§åˆ¶ç”Ÿæˆæç¤ºè¯çš„ç»†èŠ‚ç¨‹åº¦ã€‚"}),
                "negative_mode": (list(cls.NEGATIVE_PROMPTS.keys()) + ["è‡ªå®šä¹‰"], {"default": "åŸºç¡€æ¨¡æ¿", "tooltip": "é€‰æ‹©è´Ÿé¢æç¤ºè¯æ¨¡æ¿ã€‚"}),
                "custom_negative": ("STRING", {"multiline": True, "default": "", "tooltip": "å½“é€‰æ‹©â€œè‡ªå®šä¹‰â€è´Ÿé¢æ¨¡å¼æ—¶ï¼Œæ­¤å¤„å†…å®¹ç”Ÿæ•ˆã€‚"}),
            },
            "optional": { "image": ("IMAGE", {"tooltip": "(å¯é€‰) è¿æ¥å›¾ç‰‡ä»¥å¯ç”¨â€œå›¾ç”Ÿæ–‡â€æ¨¡å¼ã€‚"}) }
        }

    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("ä¼˜åŒ–åçš„æç¤ºè¯", "è´Ÿé¢æç¤ºè¯", "ä¼˜åŒ–ç¬”è®°")
    FUNCTION = "refine_prompt"
    CATEGORY = "AIæç¤ºè¯å·¥å…·"

    def _get_config_details(self, selection):
        for preset in load_preset_configs():
            if preset["name"] == selection:
                print(f"âœ… ä½¿ç”¨é…ç½®æ–‡ä»¶ '{selection}'")
                return preset
        
        raise ValueError(f"æ‰¾ä¸åˆ°åä¸º '{selection}' çš„é…ç½®ã€‚è¯·é‡å¯ComfyUIæˆ–æ£€æŸ¥config.jsonæ–‡ä»¶ã€‚")

    def _tensor_to_base64(self, tensor: torch.Tensor) -> str:
        image_np = tensor.squeeze(0).cpu().numpy()
        image_np = (image_np * 255).astype(np.uint8)
        pil_image = Image.fromarray(image_np)
        max_dim = 2048
        if pil_image.width > max_dim or pil_image.height > max_dim:
            pil_image.thumbnail((max_dim, max_dim))
            print(f"â„¹ï¸ å›¾ç‰‡å°ºå¯¸å·²è°ƒæ•´ä¸º {pil_image.size} ä»¥ç¬¦åˆAPIé™åˆ¶ã€‚")
        buffer = io.BytesIO()
        pil_image.save(buffer, format="JPEG")
        return base64.b64encode(buffer.getvalue()).decode('utf-8')

    def _get_style_map(self):
        return {
            "é€šç”¨": "General", "æ‘„å½±": "Photographic", "å†™å®": "Realistic", "åŠ¨æ¼«": "Anime", "3Dæ¨¡å‹": "3D Model", 
            "ç”µå½±æ„Ÿ": "Cinematic", "æ¦‚å¿µè‰ºæœ¯": "Concept Art", "å»ºç­‘è®¾è®¡": "Architectural", "å¥‡å¹»": "Fantasy", 
            "èµ›åšæœ‹å…‹": "Cyberpunk", "è’¸æ±½æœ‹å…‹": "Steampunk", "æ°´å¢¨ç”»": "Ink Wash Painting", "æ²¹ç”»": "Oil Painting", 
            "æ°´å½©ç”»": "Watercolor", "ç´ æ": "Sketch", "åƒç´ è‰ºæœ¯": "Pixel Art", "ä½å¤šè¾¹å½¢": "Low Poly", 
            "ç®€çº¦": "Minimalist", "å¤å¤": "Vintage"
        }

    def _build_text_system_prompt(self, style: str, detail_level: str, strict_mode: bool, target_model: str) -> str:
        strict_instruction = " Your entire response must strictly follow the format: [EN] english prompt [ZH] chinese optimization notes." if strict_mode else ""
        
        style_map = self._get_style_map()
        detail_map = {"åŸºç¡€": "Basic", "è¯¦ç»†": "Detailed", "æå…¶è¯¦ç»†": "Ultra Detailed"}
        
        style_en = style_map.get(style, 'General')
        detail_en = detail_map.get(detail_level, 'Basic')

        # --- æ ¸å¿ƒä¿®æ­£: æ˜ç¡®å‘ŠçŸ¥AIå·¥ä½œç¯å¢ƒå¹¶ç¦æ­¢å¤–éƒ¨å‚æ•° ---
        ecosystem_context = (
            "You are an expert prompt engineer for the **ComfyUI / Stable Diffusion ecosystem**. "
            "Your output will be used directly in these systems. "
            "**Crucially, do NOT include any platform-specific parameters like `--ar`, `--v`, `--style`, etc.** "
            "Focus only on the descriptive text part of the prompt."
        )

        if target_model == "Flux":
            prompt_style_instructions = (
                "You are generating a prompt for the **Flux model**. "
                "This model excels at understanding natural, descriptive language. "
                "Your task is to enhance the user's idea into a rich, descriptive prompt that works best for Flux."
            )
        elif target_model == "SDXL":
            prompt_style_instructions = (
                "You are generating a prompt for the **SDXL model**. "
                "This model understands detailed descriptions and keyword-based prompts well. "
                "Your task is to enhance the user's idea into a powerful and effective prompt that works best for SDXL."
            )
        else:
             prompt_style_instructions = (
                "You are generating a prompt for **general Stable Diffusion models (v1.5, etc.)**. "
                "These models respond best to structured, keyword-rich prompts with clear tags. "
                "Your task is to enhance the user's idea into a powerful and effective prompt in this style."
            )

        style_instruction = (
            f"It is crucial that the new prompt strongly reflects the '{style_en}' artistic style. "
            f"Infuse specific keywords, artist names, or descriptive phrases characteristic of the '{style_en}' style."
        )
        if style_en == "General":
             style_instruction = "Your goal is to create a universally effective and detailed prompt."

        return (
            f"{ecosystem_context}\n\n"
            "Your primary task is to take a user's basic idea and transform it into a highly effective prompt based on the user's chosen target model style."
            f"{strict_instruction}\n\n"
            "## Core Instructions\n"
            f"{prompt_style_instructions}\n\n"
            "## Task\n"
            "1. Rewrite the user's idea into a prompt following all the Core Instructions for the chosen style.\n"
            f"2. {style_instruction}\n"
            f"3. The level of detail should be '{detail_en}'.\n"
            "4. Provide the final English prompt and Chinese optimization notes in the required format."
        )

    def refine_prompt(self, prompt: str, config_selection: str, target_model: str, strict_mode: bool, style: str, 
                        detail_level: str, negative_mode: str, custom_negative: str, 
                        image: torch.Tensor = None) -> Tuple[str, str, str]:
        try:
            config = self._get_config_details(config_selection)
            service_type = config.get("type")
            final_key = config.get("api_key")
            api_url = config.get("api_base")
            model = config.get("model")
            
            temp_config = SERVICE_CONFIG.get(service_type, SERVICE_CONFIG["ChatGPT"]) 
            temperature = temp_config["temperature_map"].get(detail_level, 0.5)

            headers = {"Content-Type": "application/json"}
            payload = {}

            if image is not None:
                if service_type == "DeepSeek":
                    raise ValueError("DeepSeek åœ¨æ­¤èŠ‚ç‚¹ä¸­å½“å‰ä¸æ”¯æŒå›¾ç‰‡è¾“å…¥ã€‚è¯·ä½¿ç”¨ Gemini æˆ– ChatGPT è¿›è¡Œè§†è§‰åˆ†æã€‚")
                print("ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾ç‰‡ï¼Œåˆ‡æ¢åˆ°è§†è§‰åˆ†ææ¨¡å¼ã€‚")
                base64_image = self._tensor_to_base64(image)
                
                style_map = self._get_style_map()
                detail_map = {"åŸºç¡€": "Basic", "è¯¦ç»†": "Detailed", "æå…¶è¯¦ç»†": "Ultra Detailed"}
                style_en = style_map.get(style, 'General')
                detail_en = detail_map.get(detail_level, 'Basic')

                system_prompt_text = (
                    "You are an expert in analyzing images and creating descriptive prompts for AI image generation within the **ComfyUI / Stable Diffusion ecosystem**. "
                    "First, describe the provided image in detail. Then, use that description to generate a new, optimized prompt. "
                    f"The prompt should be suitable for a '{target_model}' model. Follow its best practices. "
                    "**Crucially, do NOT include any platform-specific parameters like `--ar`, `--v`, `--style`, etc.**"
                )
                user_prompt_text = f"Analyze the image, then create a new prompt. The desired art style is '{style_en}' and detail level is '{detail_en}'. User's additional instruction: '{prompt}'"
                
                strict_instruction = " Your entire response must strictly follow the format: [EN] english prompt [ZH] chinese optimization notes." if strict_mode else ""
                user_prompt_text += f"\n\n{strict_instruction}"

                if service_type == "Gemini":
                    if "key=" not in api_url: api_url = f"{api_url}?key={final_key}"
                    final_user_text = f"{system_prompt_text}\n\n{user_prompt_text}"
                    payload = { "contents": [{"parts": [ {"text": final_user_text}, {"inline_data": {"mime_type": "image/jpeg", "data": base64_image}} ]}], "generationConfig": {"temperature": temperature} }
                else:
                    headers["Authorization"] = f"Bearer {final_key}"
                    payload = { "model": model, "messages": [ {"role": "system", "content": system_prompt_text}, {"role": "user", "content": [ {"type": "text", "text": user_prompt_text}, {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}} ]} ], "temperature": temperature, "max_tokens": 2048 }
            else:
                print("âœï¸ æœªæä¾›å›¾ç‰‡ï¼Œåœ¨çº¯æ–‡æœ¬æ¨¡å¼ä¸‹è¿è¡Œã€‚")
                system_prompt_text = self._build_text_system_prompt(style, detail_level, strict_mode, target_model)
                if service_type == "Gemini":
                    if "key=" not in api_url: api_url = f"{api_url}?key={final_key}"
                    payload = {"contents": [{"parts": [{"text": f"{system_prompt_text}\n\nUser idea: {prompt}"}]}], "generationConfig": {"temperature": temperature}}
                else:
                    headers["Authorization"] = f"Bearer {final_key}"
                    payload = {"model": model, "messages": [{"role": "system", "content": system_prompt_text}, {"role": "user", "content": prompt}], "temperature": temperature}

            print(f"ğŸš€ æ­£åœ¨è°ƒç”¨ {service_type} API ({api_url}ï¼Œæ¨¡å‹: {model})...")
            response = requests.post(api_url, headers=headers, data=json.dumps(payload), timeout=180)
            response.raise_for_status()

            try:
                data = response.json()
            except requests.exceptions.JSONDecodeError:
                error_text = response.text
                print(f"âŒ è§£æJSONå¤±è´¥ï¼æœåŠ¡å™¨è¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚")
                print(f"ğŸ‘‡ æœåŠ¡å™¨åŸå§‹å“åº”å†…å®¹: \n---\n{error_text}\n---")
                raise ValueError(f"æœåŠ¡å™¨è¿”å›å†…å®¹æ— æ³•è§£æï¼Œè¯·æ£€æŸ¥APIåœ°å€æˆ–å¯†é’¥æ˜¯å¦æ­£ç¡®ã€‚")

            tokens_used_text = ""
            if service_type in ["ChatGPT", "DeepSeek"]:
                if 'usage' in data and 'total_tokens' in data['usage']:
                    tokens_used_text = f"Tokens: {data['usage']['total_tokens']}. "
                    print(f"ğŸª™ Tokens å·²ä½¿ç”¨: {data['usage']['total_tokens']}")
            
            content = data['candidates'][0]['content']['parts'][0]['text'] if service_type == "Gemini" else data['choices'][0]['message']['content']
            
            en_part_match = re.search(r"\[EN\](.*?)\[ZH\]", content, re.DOTALL)
            optimized_prompt = clean_text(en_part_match.group(1)) if en_part_match else "âš ï¸ è§£æä¼˜åŒ–åçš„æç¤ºè¯å¤±è´¥ã€‚æ£€æŸ¥AIæ˜¯å¦è¿”å›äº†[EN]...[ZH]...æ ¼å¼ã€‚"
            
            optimization_notes = content
            
            negative_prompt_text = clean_text(custom_negative) if negative_mode == "è‡ªå®šä¹‰" else self.NEGATIVE_PROMPTS.get(negative_mode, "")
            
            print("ğŸ‰ æç¤ºè¯ä¼˜åŒ–å®Œæˆï¼")
            return (optimized_prompt, negative_prompt_text, optimization_notes)

        except Exception as e:
            error_msg = f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"
            print(error_msg)
            return (prompt, "", error_msg)

NODE_CLASS_MAPPINGS = { "AIPromptRefiner": AIPromptRefiner }
NODE_DISPLAY_NAME_MAPPINGS = { "AIPromptRefiner": "AIæç¤ºè¯ä¼˜åŒ–å™¨" }
